---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Christy Cao"
date: 'May 1, 2020'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

# Modeling

- **0. (5 pts)** Introduce your dataset and each of your variables (or just your main variables if you have lots) in a paragraph. What are they measuring? How many observations?

I will be working with my datasets that I worked with in Project 1. I've added data set #4: US States Production, data set #5: the highest reporting type of exercise for adults in each state, data set #6: the obesity levels reported in each state, and data set #7: the death penalty status for each state. Health has many intersectional compartments so for this project, I am aiming to see if there is relationship between the resources of a state (economically, socially, etc.) and health outcomes. Overall, I will be analyzing the socioeconomic factors of each state and trying to find relationships on how these socioeconomic factors shape the lives of people in the different states. I will be attempting to see if these factors affect people's health, and be drawing from project 1 where I assessed the more political side of each state. After combining the datasets, it appeared that there were NAs, so I omitted those states with NAs.

```{r}
#Data set #1: Violent Crime Rates by US State
library(readr)
crime <- read_csv("USArrests.csv")

#Data set #2: Road Accident Deaths in US States
pov <- read_csv("est18ustheOne.csv", 
    skip = 1)
head(pov)
#Data set #3: Political party of each state 
pol <- read_csv("stateparty.csv", 
    col_types = cols(`Political Party` = col_character(), 
        State = col_character()))
head(pol)

#Data set #4: US States Production
produc <- read_csv("Produc.csv")


#Data
exercise <- read_csv("exercise.csv")


library(dplyr)
#data
adult_obese<-read_csv("adult_obese.csv")
adult_obese<-adult_obese%>%mutate(Location=tolower(Location))%>%select(Location,Value)%>%na.omit()


#binary data
deathpenalty <- read_csv("deathpenalty.csv")


library(tidyverse)
##Merging datasets for a final one 
crime<-crime%>%select(-UrbanPop)
pov<-pov%>%select("Name", "Poverty Percent, All Ages")
together1<-full_join(crime,pov, by=c("State"="Name"))
glimpse(together1)
together2<-full_join(together1, pol, by=c("State"="State"))
together2<-together2%>%mutate(State=tolower(State))
produc<-produc%>%mutate(state=tolower(state))
together3<-full_join(together2,produc, by=c("State"="state"))
together3<-together3%>%na.omit(state)
together4<-full_join(together3,exercise, by=c("State"="LocationAbbr"))
together<-full_join(together4,adult_obese, by=c("State"="Location"))
together<-together%>%na.omit()

#dataset for binary code
togetherbinary<-full_join(together,deathpenalty, by=c("State"="State"))
view(togetherbinary)
```


- **1. (15 pts)** Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all doesn't make sense) show a mean difference across levels of one of your categorical variables (3). If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). Briefly discuss assumptions and whether or not they are likely to have been met (2).

```{r}
library(dplyr)
library(tidyverse)
man1<-manova(cbind(Murder,Assault, Rape, `Poverty Percent, All Ages`,pcap,water,util,pc,gsp,emp,unemp,Data_Value,Value)~`Short_Desc`,data=together)
summary(man1)
summary.aov(man1)
together%>%group_by(Short_Desc)%>%summarize(mean(`Poverty Percent, All Ages`), mean(Data_Value), mean(Value))
pairwise.t.test(together$`Poverty Percent, All Ages`, together$Short_Desc, p.adj = "none")
pairwise.t.test(together$`Data_Value`, together$Short_Desc, p.adj = "none")
pairwise.t.test(together$Value, together$Short_Desc, p.adj = "none")
#1 MANOVA, 13 ANOVAS, 30 T-TESTS = 34 total tests 
1-((.95)^34)
.05/34
```
After performaning a MANOVA test, it appeared that at least one of my numerical variables show a mean difference across my categorical value, "short_desc", which reports the main type of exercise for adults in each state. Univariate ANOVAs reported the only significant numerical variables were: Poverty percent, data_value(the percent of exercise reported), and value (the percent of obese adults).

Post hoc t-tests were performed to see which groups differed. With an alpha value of .05, poverty percentages differed for states who mainly performed aerobic exercises to no exercise; aerobic + strength to high aerobic, no exercise, and strength; high aerobic to no exercise, and no excerise to strength.With an alpha value of .05, the proportion of exercise differed for aerobic to aerobic + strength, high aerobic, no exercise, and strength; aerobic + strength to high aerboic, no exercise, and strength. The groups differed for obesity levels for aerobic to no exercise, aerobic + strength to no exercise, high aerobic to no exercise and no exercise to strength.

I performed 1 MANOVA test, 13 univariate ANOVA tests, and 30 t-tests, totalling to 34 tests. The probability of at least one type I error is .825, or 82.5%. 
The significance level after the adjusted bonferroni correction is 0.001470588. After the correction, for poverty percentage, aerobic + strength to no exercise and strength was no longer significant. For the proportion of exercise and obesity levels, all comparisons remained significant. 

Assumptions were most likely met because the sample was random and independent, had a large sample size so normality was most likely met. However, equal variance may not have been met between the different groups. 



- **2. (10 pts)** Perform some kind of randomization test on your data (that makes sense). This can be anything you want! State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).

```{R}

library(vegan)
library(ggplot2)
dists<- together%>%select(`Poverty Percent, All Ages`, Data_Value, Value)%>%dist()
adonis(dists~Short_Desc, data=together)
{hist(dists); abline(v=39.92, col="red",add=T)}
```
I performed a randomization-test MANOVA -- a PERMANOVA because it allows for differences in variance and isn't sensitive to outliers. 

The null hypothesis: There are no differences in the centroids among groups, given any differences in within group dispersions.
Alternative hypothesis: There are differences in the centroids among groups/ the spread of the objects is different between the groups. 
A histogram with the test statistic was plotted. 


- **3. (35 pts)** Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.

    - Interpret the coefficient estimates (do not discuss significance) (10)
    - Plot the regression using `ggplot()`. If your interaction is numeric by numeric, refer to code near the end of WS15 to make the plot. If you have 3 or more predictors, just chose two to plot for convenience. (8)
    - Check assumptions of linearity, normality, and homoskedasticity either graphically or using a hypothesis test (4)
    - Regardless, recompute regression results with robust standard errors via `coeftest(..., vcov=vcovHC(...))`. Discuss significance of results, including any changes from before/after robust SEs if applicable. (8)
    - What proportion of the variation in the outcome does your model explain? (4)
    
```{r}
library(lmtest)
library(sandwich)
together$gsp_c <- together$gsp - mean(together$gsp)
fit1<-lm(Value~gsp_c*`Political Party` , data=together)
summary(fit1)
ggplot(together,aes(gsp_c,Value)) + geom_point(aes(color=`Political Party`)) + geom_smooth(method="lm")
#checking assumptions
resids<-fit1$residuals
fitvals<-fit1$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
ggplot()+geom_histogram(aes(resids),bins=10)
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids), color='red')
#robust standard errors
coeftest(fit1, vcov = vcovHC(fit1))
```
Predicted obesity levels for democratic states is 2.84e1, controlling for an average gross state product.

Controlling for a state's political party, there is a decrease of 3.659e-6 in obesity levels for every 1 unit increase in gross state product on average. 
Controlling for average gross state product, there is an increase in 2.320 in obesity levels for republican states.
The slope for gsp on obesity levels is 2.125e-6 higher for republican states compared to democratic states. 

I checked assumptions by making a graph of residuals for homoskedasticity, histogram for normality, and making a qqplot for linearity. It appears that the dataset fits the assumptions.   

After robust standard errors were computed, there were no changes in significant p-values. 
My model explains 9.1% of the proportion of variation in the outcome (not that great...).

- **4. (5 pts)** Rerun same regression model (with interaction), but this time compute bootstrapped standard errors. Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)

```{r}
library(dplyr)
samp_distn<-replicate(5000, {
boot_dat <- sample_frac(together, replace=T) #bootstrap your data
fit1 <- lm(Value~gsp_c*`Political Party` , data=boot_dat) #fit model
coef(fit1) #save coefs
})
## Estimated SEs
samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)
summary(fit1)
```
With the bootstrapped standard errors, there were little changes in SEs but the coefficients changed and all became significant. Comparing the SEs, the intercept boostrapped SE decreased while the gsp_c, political party Republican, and the gsp_c: Political Party Republican coefficients boostrapped SEs increased. 

- **5. (40 pts)** Perform a logistic regression predicting a binary categorical variable (if you don't have one, make/get one) from at least two explanatory variables (interaction not necessary). 

    - Interpret coefficient estimates in context (10)
    - Report a confusion matrix for your logistic regression (2)
    - Compute and discuss the Accuracy, Sensitivity (TPR), Specificity (TNR), and Recall (PPV) of your model (5)
    - Using ggplot, plot density of log-odds (logit) by your binary outcome variable (3)
    - Generate an ROC curve (plot) and calculate AUC (either manually or with a package); interpret (10)
    - Perform 10-fold (or repeated random sub-sampling) CV and report average out-of-sample Accuracy, Sensitivity, and Recall (10)
    
```{r}
data<-togetherbinary%>%mutate(y=ifelse(Death=="yes",1,0))
fit2<-glm(y~`Poverty Percent, All Ages`+`Political Party`,data=data,family="binomial")
coeftest(fit2)
prob<-predict(fit2,type="response")
pred<- ifelse(prob>.5,1,0)
table(truth=data$y, prediction=pred)%>%addmargins

#accuracy = proportion of correctly identified cases
(19+12)/46
#sensitivity (tpr) 
19/27
#specificity (tnr) 
12/19
#recall (ppv)
19/26

#ggplot for logodds

data$logit<-predict(fit2)
ggplot(data,aes(logit, fill=Death)) + geom_density(alpha=.3) + geom_vline(xintercept=0,lty=2)


#ROC AND AUC
sens<-function(p,data=data, y=y) mean(data[data$y==1,]$prob>p)
spec<-function(p,data=data, y=y) mean(data[data$y==0,]$prob<p)
sensitivity<-sapply(seq(0,1,.01),sens,data)
specificity<-sapply(seq(0,1,.01),spec,data)
ROC1<-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))
ROC1%>%gather(key,rate,-cutoff)%>%ggplot(aes(cutoff,rate,color=key))+geom_path()+
geom_vline(xintercept=c(.1,.5,.9),lty=2,color="gray50")
ROC1$TPR<-sensitivity
ROC1$FPR<-1-specificity
library(plotROC)
ROCplot<-ggplot(data)+geom_roc(aes(d=data$y,m=prob), n.cuts=0)
ROCplot


#ACC,TPR,TNR,PPV,AUC
truth<-data$y
class_diag<-function(prob,truth){

tab<-table(factor(prob>.5,levels=c("FALSE","TRUE")),truth)
tab
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]

if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1

#CALCULATE EXACT AUC
ord<-order(prob, decreasing=TRUE)
prob <- prob[ord]; truth <- truth[ord]

TPR=cumsum(truth)/max(1,sum(truth)) 
FPR=cumsum(!truth)/max(1,sum(!truth))

dup<-c(prob[-1]>=prob[-length(prob)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)

n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

data.frame(acc,sens,spec,ppv,auc)
}

class_diag(prob,truth)

library(slackr)
##10fold
set.seed(1234)
k=5

data1<-data[sample(nrow(data)),] #put dataset in random order
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){          # FOR EACH OF 10 FOLDS
  train<-data1[folds!=i,] # CREATE TRAINING SET
  test<-data1[folds==i,]  # CREATE TESTING SET
  truth<-test$y
  train
  fit<-glm(y~`Poverty Percent, All Ages`+`Political Party`,data=train,family="binomial")
  prob<- predict(fit, newdata=test, type="response")
  
  diags<-rbind(diags,class_diag(prob,truth)) #CV DIAGNOSTICS FOR EACH FOLD
}
summarize_all(diags,mean)

```
Intercept: Odds of death penalty for a democratic state, controlling for poverty percentage is -4.56654. 
Poverty Percent, All Ages: Controlling for political party, for every one increase in poverty percentage, odds of the state having a death penalty increases by a factor or .33585. 
Political party, republican: Controlling for poverty percentage, for everyone one increase in a state being republican, the odds of the state having a death penalty increases by 1.43774.
Reported confusion matrix: 

   prediction
truth  0  1 Sum
  0   12  7  19
  1    8 19  27
  Sum 20 26  46
  
  
For my model, the accuracy (proportion of correctly identified cases) is 0.673913. The sensitivity (true positive rate) is 0.7037037. The specificity (true negative rate) is 0.6315789. The recall is 0.7307692. 

A ROC plot was generated and the AUC was calculated to be .7846004, which is fair, and tells us the probability that a randomly selected y=1 has a higher predicted probability than a randomly selected y=0. 

A 10-fold CV was performed. The accuracy was .67, the sensitivity was .725, the recall was .7167, and the auc was .751.

- **6. (10 pts)** Choose one variable you want to predict (can be one you used from before; either binary or continuous) and run a LASSO regression inputting all the rest of your variables as predictors. Choose lambda to give the simplest model whose accuracy is near that of the best (i.e., `lambda.1se`). Discuss which variables are retained. Perform 10-fold CV using this model: if response in binary, compare model's out-of-sample accuracy to that of your logistic regression in part 5; if response is numeric, compare the residual standard error (at the bottom of the summary output, aka RMSE): lower is better fit!

```{r}
library(glmnet)
set.seed(1234)
data1<-data%>%select(-State, -year,-Class,-logit,-`Political Party`,-Short_Desc)
data2<-data1%>%na.omit()%>%as.matrix()
x<-model.matrix(y~.,data=data1)
scale(x)
y<-as.matrix(data1$y)
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)

set.seed(1234)
k=10 #choose number of folds
data3<-data1[sample(nrow(data1)),] #randomly order rows
folds<-cut(seq(1:nrow(data1)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
## Create training and test sets
train<-data3[folds!=i,]
test<-data3[folds==i,]
truth<-data3$y

y<-as.matrix(data3$y)
x<-model.matrix(y~Death,data=data3)
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
lassoprob<- predict(lasso,newx=x,type="response")
## Test model on test set (save all k results)
diags<-rbind(diags,class_diag(lassoprob,truth))
}
diags%>%summarize_all(mean)

```
The only variables that are retained was the Deathyes variable. The out of sample accuracy (with k=5, k=10 didn't work) came out to be 1. I think the out of sample performance was a lot higher because using Lasso, only the variable that was retained was "Deathyes", so the other variables that were tested in the logistic regression weren't tested, making the out of sample accuracy much higher. 






